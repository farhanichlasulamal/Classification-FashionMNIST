"""Classification_FashionMNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VWGsWf0ZzUZYblGAKnUHImfBd9EIeV1s
"""

# class FashionMNIST(Dataset):
#     def __init__(self, sets='train'):
#         super(FashionMNIST, self).__init__()
#         data = np.genfromtext('./drive/My Drive/FashionMNIST/'+sets+'.csv', delimiter=',')
#         self.labels = data[:,0]
#         self.images = data[:,1:].reshape((-1,28,28))
 
#     def __getitem__(self, index):
#         image = self.images[index,:,:]
#         tensor_image = torch.from_numpy(image)
#         return tensor_image
    
#     def __len__(self):
#         return self.labels.size

import os
import csv
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import numpy as np

from math import ceil
from tqdm.notebook import tqdm

from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from torchvision.datasets import FashionMNIST

SEED = 4444
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

NUM_EPOCH = 100
NUM_BATCH = 0
BATCH_SIZE = 256
ITERATION = None
classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',
               'Shirt', 'Sneaker', 'Bag', 'Ankle boot')
 
class ClassifierNet(nn.Module):
    def __init__(self):
        super(ClassifierNet, self).__init__()
        layers = [
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=3, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(in_features=576, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=64),
            nn.ReLU(),
            nn.Linear(in_features=64, out_features=10)
        ]
        self.layers = nn.Sequential(*layers)
        
    def forward(self, x):
        y = self.layers(x)
        return y
 
def train(net, loss_fn, optimizer, train_loader, epoch, log_dir):
    global ITERATION
    ITERATION = NUM_BATCH * (epoch-1) + 1
    net.train()
    train_bar = tqdm(train_loader)
    batch_sizes = 0
    total_loss  = 0
    
    for image, label in train_bar:
        batch_size = label.size(0)
        batch_sizes += batch_size

        image = image.cuda()
        label = label.cuda()

        net.zero_grad()
        prediction = net(image)

        loss = loss_fn(prediction, label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_size
        train_bar.set_description(desc='itr:%d [%3d/%3d] Loss: %.8f' %(ITERATION, epoch, NUM_EPOCH, total_loss/batch_sizes))
        write_csv(log_dir+'loss_log_{}.csv'.format(epoch), [ITERATION, loss.item()])
        ITERATION = ITERATION + 1
                
    torch.cuda.empty_cache()
 
def main(checkpoint=None, batch_size=BATCH_SIZE):
    train_set = FashionMNIST(root='./data', train=True, transform = transforms.Compose([transforms.ToTensor()]), download=True)
    test_set = FashionMNIST(root='./data', train=False, transform = transforms.Compose([transforms.ToTensor()]), download=True)
    train_loader = DataLoader(dataset=train_set, num_workers=16, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(dataset=train_set, num_workers=16, batch_size=batch_size, shuffle=False)
    global NUM_BATCH
    NUM_BATCH = ceil(len(train_loader.dataset)/batch_size)

    net = ClassifierNet().cuda()
    loss = nn.CrossEntropyLoss().cuda()
    optimizer = optim.Adam(net.parameters(), lr=0.0001)

    log_dir = './drive/My Drive/FashionMNIST/logs'
    checkpoint_dir = './drive/My Drive/FashionMNIST/checkpoint'
    test_dir = './drive/My Drive/FashionMNIST/test imgs'

    mkdir(log_dir)
    mkdir(checkpoint_dir)
    mkdir(test_dir)

    if checkpoint is not None:
        load_checkpoint(checkpoint_dir, net, optimizer, checkpoint)
    else:
        checkpoint = 0

    for epoch in range(checkpoint+1, NUM_EPOCH+1):
        train(net, loss, optimizer, train_loader, epoch, log_dir)
        save_checkpoint(checkpoint_dir, net, optimizer, epoch)
        with torch.no_grad():
            if epoch % 20 == 0 or epoch < 5:
                accuracy = validate(net, test_dir, epoch, test_loader)
                write_csv(log_dir + 'accuracy_validation_log.csv', [ITERATION, accuracy])

def validate(net, path_dir, epoch, data_loader):
    correct = 0
    total = 0

    net.eval()
    for image, label in data_loader:
        image = image.cuda()
        label = label.cuda()
        
        outputs = net(image)
        _, pred = torch.max(outputs.cpu().detach().data, 1)
        
        total += label.cpu().detach().size(0)
        correct += (pred == label.cpu().detach()).sum().item()
        torch.cuda.empty_cache()
    
    accuracy = 100 * correct / total
    print('Accuracy on the ' + str(total) + ' test set: {} %'.format(accuracy))
    return accuracy

def mkdir(path):
    try:
        os.mkdir(path)
    except:
        pass

def save_checkpoint(checkpoint_dir, model, optim, epoch):
    save_path = checkpoint_dir + '/epoch_{}.pth'.format(epoch)
    state = {'epoch': epoch, 'model': model, 'optim': optim}
    torch.save(state, save_path)
    print("Checkpoint saved to {}".format(save_path))

def load_checkpoint(checkpoint_dir, net, optimizer, epoch):
    load_path = checkpoint_dir + '/epoch_{}.pth'.format(epoch)
    weights = torch.load(load_path)
    net.load_state_dict(weights['model'].state_dict())
    if optimizer is not None:
        optimizer.load_state_dict(weights['optim'].state_dict())

def change_lr(optim, lr):
    for param in optim.param_groups:
        param['lr'] = lr

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

def write_csv(file, data):
    with open(file, 'a', newline='') as outfile:
        writer = csv.writer(outfile)
        writer.writerow(data)

if __name__ == '__main__':
    main()
    pass